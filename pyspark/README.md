**This folder details my experience with using pySpark in the context of academic projects. It includes the following:**
- ## Analysing Road Crash Data
  - Create RDDs
  - Convert these into Key-Value Pair RDDs
  - Partition RDDs based on their Key
  - Create RDD operations to answer a number of queries 
  - Create PySpark DataFrames
  - Create DataFrame operations to answer a number of queries  
  - Instantiate a Spark SQL session
  - Create Spark SQL operations to answer a number of queries
<br/>

- ## Detecting Linux System Hacking Activities
  - Specify and enforce a data schema by rectifying data entry errors
  - Explore the dataset
    - Detect class imbalances
    - Calculate basic statistics (count, mean, standard deviation, minimum, maximum)
    - Visualise data (histograms, correlation plots)
  - Create transformers and estimators to perform one-hot encoding, and string indexing to prepare the data for a Machine Learning pipeline
  - Preare a decision tree, and a gradient-boosted tree using the transformed and pre-processed dataset
  - Evaluate models by considering their accuracy, recall, and precision
